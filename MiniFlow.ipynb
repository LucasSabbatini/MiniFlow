{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniFlow\n",
    "\n",
    "In this lesson, I'll build a miniflow, a module that stores a simple neural network, implemented in python using numpy. The structure of this code was created by instructors from Udacity's Deep Learning Foundation, while some implementation as created by me as exercises for the Nanodegree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniFlow Architecture\n",
    "\n",
    "A Python class we'll ve used to represent a generic node. Each node will receive input from multiple other nodes, and also creates a single output that will likely be passed to other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes=[]):\n",
    "        # Node(s) from which this node receives values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        \n",
    "        # Node(s) to which this node passes values\n",
    "        self.outbound_nodes = []\n",
    "        \n",
    "        # For each inbound nodes here, add this node as an inbound node to that node\n",
    "        for node in self.inbound_nodes:\n",
    "            node.outbound_nodes.append(self)\n",
    "            \n",
    "        # Initializing the value that will be passed to other nodes as None\n",
    "        self.value = None\n",
    "         \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        \n",
    "        Compute the output value based on inbound_nodes and\n",
    "        store the result in self.value. It doesn't actually perform the forward pass,\n",
    "        only calculate its value and stores it in self.value\n",
    "        \"\"\"\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class node only set the base set of properties every node holds, but only specialized subclasses of Node will end up in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        # Since the input is the first node in the graph, it has no inbound_nodes\n",
    "        # so, when initializing the Node class, there's no need to pass in any other nodes\n",
    "        Node.__init__(self)\n",
    "        \n",
    "    def forward(self, value=None):\n",
    "        \"\"\"\n",
    "        This is the only node where the value may be passe din as an argument for\n",
    "        forward method, since it does not have to perform and operation with values from inbound nodes\n",
    "        \"\"\"\n",
    "        # Remember that self.value was already initiated in Node.__init__(self)\n",
    "        # Overwrite the value if one is passed in\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Add Subclass\n",
    "\n",
    "The Add subclass actually performs a calculation, addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "named arguments must follow bare * (<ipython-input-53-67ee5fb841f4>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-53-67ee5fb841f4>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    def __init__(self, *):\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m named arguments must follow bare *\n"
     ]
    }
   ],
   "source": [
    "class Add(Node):\n",
    "    \"\"\"\n",
    "    This class will take a list of nodes and add the values stored in them together\n",
    "    \"\"\"\n",
    "    def __init__(self, *):\n",
    "        \"\"\"\n",
    "        We'll initialize the Node (parent) init function with the arguments given \n",
    "        in to the Add class initialization. We'll pass them as a list, since the parent\n",
    "        node has a list as an argument for inbound_nodes\n",
    "        \"\"\"\n",
    "        Node.__init__(seld, list(inputs))\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        This method will add the values stored in inbound_nodes\n",
    "        \"\"\"\n",
    "        summ = 0\n",
    "        for node in self.inbound_nodes:\n",
    "            summ += node.value\n",
    "        self.value = summ\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the input of some node depends on the output of other, there are dependencies for the order of the operations. To arrange the nodes in a order such that the operations can be performed, we'll have to sort the nodes before applying the forward pass.\n",
    "The topological_sort() function implements topological sorting using Kahn's Algorithm. This function returns a sorted list of nodes in which all of the calculations can run in series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "    \n",
    "    Arguments:\n",
    "    'output_node': The output of the graph (no outgoing edges).\n",
    "    'sorted_nodes': a topologically sorted list of nodes.\n",
    "    \n",
    "    Returns the output node's value\n",
    "    \"\"\"\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "        \n",
    "    return output_node.value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be defined the topological_sort() function, which implements topological sorting using Kahn's Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "We'll now use the structures created above to perform a forward pass in our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "f = Add(x, y, z)\n",
    "\n",
    "feed_dict = {x:4, y: 5, z:10}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(\"{} + {} + {} = {} (according to miniflow)\".format(x.value, y.value, z.value, output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and Loss\n",
    "\n",
    "So far, this neural network can only perform a forward pass through its nodes. The final objective is to improve the accuracy of their outputs over time, which is not useful for an Add node. So before we dive into the backpropagation part, a more complex node shall be implemented, the Linear node, which will perform a linear combination of a input nodes list, weights list and a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "        # Since Linear node intantiate a regular Node with a list of lists, the\n",
    "        # self.inbound_nodes isn't a list of nodes, but a list of three things:\n",
    "        # list o inbound nodes, list of weights, and a bias\n",
    "        \n",
    "    def forward(self):\n",
    "        linear_combination = 0\n",
    "        \n",
    "        for i in range(len(self.inbound_nodes[0].value)):\n",
    "            linear_combination += self.inbound_nodes[0].value[i] * self.inbound_nodes[1].value[i]\n",
    "        linear_combination += self.inbound_nodes[2].value\n",
    "        \n",
    "        self.value = linear_combination \n",
    "        \n",
    "        \"\"\"\n",
    "        Using iteration with python structure is computationally poor, it's worth to mention\n",
    "        that those python lists could have been transformed to python arrays and the dot product\n",
    "        of them been performed to obtain the linear combination\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward_pass() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-6422e45a1c52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopological_sort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward_pass() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {inputs:[6, 14, 3],\n",
    "            weights:[0.5, 0.25, 1.4],\n",
    "            bias: 2}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example was performed with only one data point as input. Usually, it's common to feed in multiple data points in each forward pass, because the linear combinations can be processed in parallel, resulting in performance gains. The number of data points (exemples) is called batch size, and common numbers for batch size are 32, 64, 128, 256, 512.\n",
    "\n",
    "So now the previous Linear node will be addapted to perform linear a linear transformation of the input matrix, which will be of size m (number of data points) by n (number of features of each exemple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        Node.__init__(self, [X, W, b])\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        This method will now perform a matrix multiplication between the features matrix\n",
    "        and the weights matrix, and later add the bias array\n",
    "        \"\"\"\n",
    "        inputs = self.inbound_nodes[0].value # m x n numpy matrix\n",
    "        weights = self.inbound_nodes[1].value # n x k numpy matrix\n",
    "        bias = self.inbound_nodes[2].value # vector of size k\n",
    "        \n",
    "        linear_transform = np.dot(inputs, weights)\n",
    "        linear_transform += bias\n",
    "        \n",
    "        self.value = linear_transform\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W:W_, b:b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Linear transforms are great for simply shifting values, but neural networks often require a more nuanced transform. For instance, one of the original designs for an artificial neuron, the perceptron, exhibit binary output behavior. Perceptrons compare a weighted input to a threshold. When the weighted input exceeds the threshold, the perceptron is activated and outputs 1, otherwise it outputs 0.\n",
    "\n",
    "Activation, the idea of binary output behavior, generally makes sense for classification problems. For example, if you ask the network to hypothesize if a handwritten image is a '9', you're effectively asking for a binary output - yes, this is a '9', or no, this is not a '9'. A step function is the starkest form of a binary output, which is great, but step functions are not continuous and not differentiable, which is very bad. Differentiation is what makes gradient descent possible.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "\n",
    "As quoted above from the lesson of Deep Learning foundation course on MiniFlow, we need to use a differentiable funciton for the activation, so we can find a way to update the weights, in this case, using the gradient of the funtion. \n",
    "\n",
    "A sigmoid node must be created, one that takes as input a Linear node, perform and stores in its value the sigmoid of the input's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    You need to fix the `_sigmoid` and `forward` methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used later with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        return (1/(1+np.exp(-x)))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the result of the\n",
    "        sigmoid function, `_sigmoid`.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        # This is a dummy value to prevent numpy errors\n",
    "        # if you test without changing this method.\n",
    "        \n",
    "        \n",
    "        self.value = self._sigmoid(self.inbound_nodes[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost\n",
    "\n",
    "There are still some structures needed to make a neural network learn upon the data it is given in. To measure how well the prediction of the network is, we'll use the Mean Square Error function, which evaluates how far your predicionts are form the correct labeled data from your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        Node.__init__(self, [y, a])\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        since the matrices might not be of the same shape, it is needed to reshape them so they\n",
    "        can be broadcasted together with no error\"\"\"\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        error = y-a\n",
    "        squared_sum = np.square(error).sum()\n",
    "        self.value = squared_sum/y.shape[0]\n",
    "        \n",
    "def forward_pass(graph):\n",
    "    for n in graph:\n",
    "        n.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
